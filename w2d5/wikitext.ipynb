{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install transformers torch einops fancy_einsum wandb plotly torchinfo tqdm --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf<=3.18 in /home/ubuntu/.local/lib/python3.8/site-packages (3.17.3)\n",
      "Requirement already satisfied: six>=1.9 in /usr/lib/python3/dist-packages (from protobuf<=3.18) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install 'protobuf<=3.18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import utils\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import wandb\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "DATA_FOLDER = \"./data\"\n",
    "DATASET = \"2\"\n",
    "BASE_URL = \"https://s3.amazonaws.com/research.metamind.io/wikitext/\"\n",
    "DATASETS = {\"103\": \"wikitext-103-raw-v1.zip\", \"2\": \"wikitext-2-raw-v1.zip\"}\n",
    "TOKENS_FILENAME = os.path.join(DATA_FOLDER, f\"wikitext_tokens_{DATASET}.pt\")\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.mkdir(DATA_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url: str, path: str) -> None:\n",
    "    '''\n",
    "    Download the file from url and save it to path. \n",
    "    If path already exists, do nothing.\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {url} to {path}\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset WikiText-2 - options are 2 and 103\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_FOLDER, DATASETS[DATASET])\n",
    "maybe_download(BASE_URL + DATASETS[DATASET], path)\n",
    "expected_hexdigest = {\"103\": \"0ca3512bd7a238be4a63ce7b434f8935\", \"2\": \"f407a2d53283fc4a49bcff21bc5f3770\"}\n",
    "with open(path, \"rb\") as f:\n",
    "    actual_hexdigest = hashlib.md5(f.read()).hexdigest()\n",
    "    assert actual_hexdigest == expected_hexdigest[DATASET]\n",
    "\n",
    "print(f\"Using dataset WikiText-{DATASET} - options are 2 and 103\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "z = zipfile.ZipFile(path)\n",
    "\n",
    "def decompress(*splits: str) -> str:\n",
    "    return [\n",
    "        z.read(f\"wikitext-{DATASET}-raw/wiki.{split}.raw\").decode(\"utf-8\").splitlines()\n",
    "        for split in splits\n",
    "    ]\n",
    "\n",
    "train_text, val_text, test_text = decompress(\"train\", \"valid\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer Modules\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the tokenizer on the list of lines with truncation=False to obtain lists of tokens. \n",
    "# These will be of varying length, and you'll notice some are empty due to blank lines.\n",
    "\n",
    "# Build one large 1D tensor containing all the tokens in sequence\n",
    "# Reshape the 1D tensor into (batch, sequence).\n",
    "\n",
    "def tokenize_1d(tokenizer, lines: List[str], max_seq: int) -> t.Tensor:\n",
    "    '''Tokenize text and rearrange into chunks of the maximum length.\n",
    "\n",
    "    Return (batch, seq) and an integer dtype.\n",
    "    '''\n",
    "    tokens = map(tokenizer.encode, lines) # tokenize\n",
    "    flattened = (item for sublist in tokens for item in sublist) # flatten\n",
    "    ten = t.tensor(list(flattened), dtype=t.int) # convert to tensor\n",
    "    ten = ten[: max_seq * (ten.shape[0] // max_seq)] # truncate to max_seq\n",
    "    return ten.reshape(-1, max_seq) # reshape to (batch, seq)\n",
    "\n",
    "if False:\n",
    "    max_seq = 128\n",
    "    print(\"Tokenizing training text...\")\n",
    "    train_data = tokenize_1d(tokenizer, train_text, max_seq)\n",
    "    print(\"Training data shape is: \", train_data.shape)\n",
    "    print(\"Tokenizing validation text...\")\n",
    "    val_data = tokenize_1d(tokenizer, val_text, max_seq)\n",
    "    print(\"Tokenizing test text...\")\n",
    "    test_data = tokenize_1d(tokenizer, test_text, max_seq)\n",
    "    print(\"Saving tokens to: \", TOKENS_FILENAME)\n",
    "    t.save((train_data, val_data, test_data), TOKENS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing empirical frequencies\n",
      "Checking fraction of tokens selected...\n",
      "Checking fraction of tokens masked...\n",
      "Checking fraction of tokens masked OR randomized...\n"
     ]
    }
   ],
   "source": [
    "def random_mask(\n",
    "    input_ids: t.Tensor, mask_token_id: int, vocab_size: int, select_frac=0.15, mask_frac=0.8, random_frac=0.1\n",
    ") -> Tuple[t.Tensor, t.Tensor]:\n",
    "    '''Given a batch of tokens, return a copy with tokens replaced according to Section 3.1 of the paper.\n",
    "\n",
    "    input_ids: (batch, seq)\n",
    "\n",
    "    Return: (model_input, was_selected) where:\n",
    "\n",
    "    model_input: (batch, seq) - a new Tensor with the replacements made, suitable for passing to the BertLanguageModel. Don't modify the original tensor!\n",
    "\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "\n",
    "    we must mask 15% of tokens in each sequence\n",
    "    80% of those must be replaced with the [MASK] token\n",
    "    The remaining 10% of masked tokens can be replaced with any token from the vocabulary.\n",
    "    '''\n",
    "    device = input_ids.device\n",
    "    d_batch, d_seq = input_ids.shape\n",
    "    n_masking = select_frac * d_seq * d_batch\n",
    "    n_mask = int(n_masking * mask_frac)\n",
    "    n_random = int(n_masking * random_frac)\n",
    "    n_unchanged = int(n_masking * (1 - mask_frac - random_frac))\n",
    "    # print(f'Out of {d_seq}, masking {n_mask} tokens with [MASK], {n_random} with random tokens, and {n_unchanged} unchanged')\n",
    "    # print(f'Frequency of [MASK] is {n_mask / d_seq}')\n",
    "    # print(f'Frequency of random is {n_random / d_seq}')\n",
    "    # print(f'Frequency of unchanged is {n_unchanged / d_seq}')\n",
    "    # choose which tokens to mask\n",
    "    mask = t.zeros(d_batch*d_seq, dtype=t.int, device=device)\n",
    "    mask[:n_mask] = 1\n",
    "    mask[n_mask:n_mask + n_random] = 2\n",
    "    mask[n_mask + n_random:n_mask + n_random + n_unchanged] = 3\n",
    "    mask = mask.reshape(d_batch, d_seq)\n",
    "    \n",
    "    # for the input ids replace them with masked token where mask = 1\n",
    "    # and with random token where mask = 2\n",
    "    model_input = input_ids.clone()\n",
    "    model_input[mask == 1] = mask_token_id\n",
    "    rand = t.randint(0, vocab_size, (d_batch, d_seq), device=device)\n",
    "    model_input = t.where(mask == 2, rand, model_input)\n",
    "    mask = t.where(mask != 0, 1, 0)\n",
    "\n",
    "    return model_input, mask\n",
    "    \n",
    "# test mask\n",
    "\n",
    "# print decoded tokens\n",
    "# print(tokenizer.decode(train_data[:2].flatten().tolist()))\n",
    "out1, mask1 = random_mask(train_data[:2], tokenizer.mask_token_id, tokenizer.vocab_size, select_frac=0.85, mask_frac=0.1, random_frac=0.8)\n",
    "# print(tokenizer.decode(out1.flatten().tolist()))\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_random_mask(random_mask, input_size=10000, max_seq=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2800)\n"
     ]
    }
   ],
   "source": [
    "# Find the word frequencies\n",
    "word_frequencies = t.bincount(train_data.flatten())\n",
    "# Drop the words with occurrence zero (because these contribute zero to cross entropy)\n",
    "word_frequencies = word_frequencies[word_frequencies > 0]\n",
    "# Get probabilities\n",
    "word_probabilities = word_frequencies / word_frequencies.sum()\n",
    "# Calculate the cross entropy\n",
    "cross_entropy = (- word_probabilities * word_probabilities.log()).sum()\n",
    "print(cross_entropy)\n",
    "# ==> 7.3446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random MLM loss on random tokens - does this make sense? 10.32\n"
     ]
    }
   ],
   "source": [
    "def flat(x: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Combines batch and sequence dimensions.\"\"\"\n",
    "    return rearrange(x, \"b s ... -> (b s) ...\")\n",
    "\n",
    "def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    pred: (batch, seq, vocab_size) - predictions from the model\n",
    "    target: (batch, seq, ) - the original (not masked) input ids\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "    Out: the mean loss per predicted token\n",
    "    '''\n",
    "    # select correct predictions from the predictions\n",
    "    target = t.where(was_selected == 1, target, -100)\n",
    "    entropy = F.cross_entropy(flat(pred), flat(target.long()))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_cross_entropy_selected(cross_entropy_selected)\n",
    "\n",
    "    batch_size = 8\n",
    "    seq_length = 512\n",
    "    batch = t.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "    pred = t.rand((batch_size, seq_length, tokenizer.vocab_size))\n",
    "    (masked, was_selected) = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "    loss = cross_entropy_selected(pred, batch, was_selected).item()\n",
    "    print(f\"Random MLM loss on random tokens - does this make sense? {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  torch.Size([19159, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "hidden_size = 512\n",
    "bert_config_tiny = TransformerConfig(\n",
    "    num_layers = 8,\n",
    "    num_heads = hidden_size // 64,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = hidden_size,\n",
    "    max_seq_len = 128,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.0002,\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "(train_data, val_data, test_data) = t.load(\"./data/wikitext_tokens_2.pt\")\n",
    "print(\"Training data size: \", train_data.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_data), shuffle=True, batch_size=config_dict[\"batch_size\"], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from fancy_einsum import einsum\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from dataclasses import dataclass \n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "import plotly.express as px\n",
    "import torchinfo\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import transformers\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import utils\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "bertconfig = TransformerConfig(\n",
    "    num_layers = 12,\n",
    "    num_heads = 12,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = 768,\n",
    "    max_seq_len = 512,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / (headsize ** 0.5)\n",
    "        mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "        scaled_dot_prod += mask_filter\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_probs = self.dropout1(attention_probs)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return self.dropout2(attention) \n",
    "\n",
    "class GPT2DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = x + self.attention(self.layernorm1(x))\n",
    "        x = x + self.mlp(self.layernorm2(x))\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_encoding = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        decoders = [GPT2DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        if len(tokens.shape) == 1:\n",
    "            tokens = rearrange(tokens, \"seq -> 1 seq\")\n",
    "        embedding = self.embed(tokens) # (b, seq_len) -> (b, seq_len, embedding)\n",
    "        pos_enc = self.positional_encoding(t.arange(tokens.shape[1], device=tokens.device)) # (seq_len)\n",
    "        a = self.dropout(embedding + pos_enc) # (b, seq_len, embedding)\n",
    "        b = self.decoderlayer(a) # (b, seq_len, embedding)\n",
    "        c = self.layernorm(b) @ self.embed.weight.T # (b, seq_len, embedding) @ (embedding, vocab_size) -> (b, seq_len, vocab_size)\n",
    "        return c\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hidden_size, num_heads = config.hidden_size, config.num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)\n",
    "        if additive_attention_mask is not None:\n",
    "            scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return attention\n",
    "\n",
    "class BERTMLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs, p_dropout = config.hidden_size, config.dropout\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(hs, hs * 4)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(hs * 4, hs)),   \n",
    "            ('dropout', nn.Dropout(p_dropout))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = BERTMLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        h1 = self.ln1(self.attention(x, additive_attention_mask) + x) # TODO chain this\n",
    "        h2 = self.ln2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')\n",
    "\n",
    "# util one is erroring\n",
    "def test_make_additive_attention_mask(make_additive_attention_mask):\n",
    "    def make_additive_attention_mask_soln(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "        '''\n",
    "        one_zero_attention_mask: \n",
    "            shape (batch, seq)\n",
    "            Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "        big_negative_number:\n",
    "            Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "        Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.\n",
    "        '''\n",
    "        return big_negative_number * repeat(1 - one_zero_attention_mask, \"batch seqK -> batch 1 1 seqK\")\n",
    "    arr = t.randint(low=0, high=2, size=(3, 4))\n",
    "    expected = make_additive_attention_mask_soln(arr)\n",
    "    actual = make_additive_attention_mask(arr)\n",
    "    t.testing.assert_close(expected, actual)\n",
    "\n",
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tokentype_emb = nn.Embedding(2, config.hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        positional_embedding = self.pos_emb(t.arange(seq_len, device=input_ids.device)) # (seq_len, emb)\n",
    "        token_type_ids = token_type_ids if token_type_ids else t.zeros_like(input_ids)\n",
    "        token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)\n",
    "        x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))\n",
    "        mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask is not None else None\n",
    "        for block in self.bertblocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs = config.hidden_size\n",
    "        self.bertcommon = BertCommon(config)\n",
    "        self.linear = nn.Linear(hs, hs)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        xavier = 1 / (config.vocab_size ** 0.5)\n",
    "        self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        unembed = self.bertcommon.token_emb.weight.T\n",
    "        x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.ln(x)\n",
    "        return x @ unembed + self.unembed_bias\n",
    "\n",
    "my_bert = BertLanguageModel(bertconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  torch.Size([19159, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "hidden_size = 512\n",
    "bert_config_tiny = TransformerConfig(\n",
    "    num_layers = 8,\n",
    "    num_heads = hidden_size // 64,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = hidden_size,\n",
    "    max_seq_len = 128,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.0002,\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "(train_data, val_data, test_data) = t.load(\"./data/wikitext_tokens_2.pt\")\n",
    "print(\"Training data size: \", train_data.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_data), shuffle=True, batch_size=config_dict[\"batch_size\"], drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model: BertLanguageModel, config_dict: dict) -> t.optim.AdamW:\n",
    "    '''\n",
    "    Loop over model parameters and form two parameter groups:\n",
    "\n",
    "    - The first group includes the weights of each Linear layer and uses the weight decay in config_dict\n",
    "    - The second has all other parameters and uses weight decay of 0\n",
    "    '''\n",
    "    params1 = []\n",
    "    params2 = []\n",
    "    matches = ['W_O.weight', 'W_V.weight', 'W_Q.weight', 'W_K.weight', 'linear1.weight', 'linear2.weight', 'linear.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if any([match in name for match in matches]):\n",
    "            params1.append(param)\n",
    "        else:\n",
    "            params2.append(param)\n",
    "    \n",
    "    # \n",
    "    # \n",
    "    params = [\n",
    "        {'params': params1, 'weight_decay': config_dict['weight_decay']},\n",
    "        {'params': params2, 'weight_decay': 0, **config_dict}\n",
    "    ]\n",
    "    return t.optim.AdamW(params, lr=config_dict['lr'])\n",
    "    \n",
    "\n",
    "if MAIN:\n",
    "    test_config = TransformerConfig(\n",
    "        num_layers = 3,\n",
    "        num_heads = 1,\n",
    "        vocab_size = 28996,\n",
    "        hidden_size = 1,\n",
    "        max_seq_len = 4,\n",
    "        dropout = 0.1,\n",
    "        layer_norm_epsilon = 1e-12,\n",
    "    )\n",
    "\n",
    "    optimizer_test_model = BertLanguageModel(test_config)\n",
    "    opt = make_optimizer(\n",
    "        optimizer_test_model, \n",
    "        dict(weight_decay=0.1, lr=0.0001, eps=1e-06)\n",
    "    )\n",
    "    expected_num_with_weight_decay = test_config.num_layers * 6 + 1\n",
    "    wd_group = opt.param_groups[0]\n",
    "    actual = len(wd_group[\"params\"])\n",
    "    assert (\n",
    "        actual == expected_num_with_weight_decay\n",
    "    ), f\"Expected 6 linear weights per layer (4 attn, 2 MLP) plus the final lm_linear weight to have weight decay, got {actual}\"\n",
    "    all_params = set()\n",
    "    for group in opt.param_groups:\n",
    "        all_params.update(group[\"params\"])\n",
    "    assert all_params == set(optimizer_test_model.parameters()), \"Not all parameters were passed to optimizer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "WARNING: No preset parameters were found for the device that Open MPI\n",
      "detected:\n",
      "\n",
      "  Local host:            150-136-220-142\n",
      "  Device name:           mlx5_0\n",
      "  Device vendor ID:      0x02c9\n",
      "  Device vendor part ID: 4126\n",
      "\n",
      "Default device parameters will be used, which may result in lower\n",
      "performance.  You can edit any of the files specified by the\n",
      "btl_openib_device_param_files MCA parameter to set values for your\n",
      "device.\n",
      "\n",
      "NOTE: You can turn off this warning by setting the MCA parameter\n",
      "      btl_openib_warn_no_device_params_found to 0.\n",
      "--------------------------------------------------------------------------\n",
      "--------------------------------------------------------------------------\n",
      "No OpenFabrics connection schemes reported that they were able to be\n",
      "used on a specific port.  As such, the openib BTL (OpenFabrics\n",
      "support) will be disabled for this port.\n",
      "\n",
      "  Local host:           150-136-220-142\n",
      "  Local device:         mlx5_0\n",
      "  Local port:           1\n",
      "  CPCs attempted:       udcm\n",
      "--------------------------------------------------------------------------\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    }
   ],
   "source": [
    "! wandb login 6732e91000e02858c92d143034dc9cfbb7eb7861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def lr_for_step(step: int, max_step: int, max_lr: float, warmup_step_frac: float):\n",
    "    '''\n",
    "    The authors used learning rate warmup from an unspecified value and an unspecified shape to a maximum of 1e-4 for the first 10,000 steps out of 1 million, and then linearly decayed to an unspecified value.\n",
    "\n",
    "    From the repo, we can see in optimization.py that AdamW is used for the optimizer, that the warmup is linear and that the epsilon used for AdamW is 1e-6.\n",
    "\n",
    "    Assume that the initial learning rate and the final learning rate are both 1/10th of the maximum, and that we want to warm-up for 1% of the total number of steps.\n",
    "    Return the learning rate for use at this step of training.'''\n",
    "    warmup_steps = int(max_step * warmup_step_frac)\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    else:\n",
    "        return max_lr * (max_step - step) / (max_step - warmup_steps)\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    max_step = int(len(train_loader) * config_dict[\"epochs\"])\n",
    "    lrs = [\n",
    "        lr_for_step(step, max_step, max_lr=config_dict[\"lr\"], warmup_step_frac=config_dict[\"warmup_step_frac\"])\n",
    "        for step in range(max_step)\n",
    "    ]\n",
    "    plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:  40425284\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63783c7b363a4d778da817167bb5333b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668917383337128, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: <ipython-input-82-5439a9e04f9f> 6 bert_mlm_pretrain\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-5439a9e04f9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mnum_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtiny_bert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of model parameters: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mbert_mlm_pretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtiny_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-82-5439a9e04f9f>\u001b[0m in \u001b[0;36mbert_mlm_pretrain\u001b[0;34m(model, config_dict, train_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interrupted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0merror_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    695\u001b[0m             )\n\u001b[1;32m    696\u001b[0m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeliver_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             result = handle.wait(\n\u001b[0m\u001b[1;32m    698\u001b[0m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_progress_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, on_probe, on_progress, release)\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mMailboxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transport failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;31m# Always update progress to 100% when done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bert_mlm_pretrain(model: BertLanguageModel, config_dict: dict, train_loader: DataLoader) -> None:\n",
    "    '''Train using masked language modelling.'''\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    opt = make_optimizer(model, config_dict)\n",
    "    wandb.init(project=\"bert\", config=config_dict)\n",
    "    wandb.watch(model)\n",
    "    run_name = wandb.run.name\n",
    "    # tqdm progress bar of train loader annotated with epoch number\n",
    "    os.makedirs(f\"models/{run_name}\")\n",
    "    t.save(model.state_dict(), f\"./models/{run_name}/epoch_-1.pt\")\n",
    "    \n",
    "    for epoch in range(config_dict['epochs']):\n",
    "        train_loader = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for n_batch, (batch,) in enumerate(train_loader):\n",
    "            wandb.log({\"epoch\": epoch, \"batch\": n_batch})\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            lr = lr_for_step(\n",
    "                n_batch + epoch * len(train_loader),\n",
    "                max_step=int(len(train_loader) * config_dict[\"epochs\"]),\n",
    "                max_lr=config_dict[\"lr\"],\n",
    "                warmup_step_frac=config_dict[\"warmup_step_frac\"],\n",
    "            )\n",
    "            opt.lr = lr\n",
    "            masked_input_ids, mask = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "            masked_input_ids = masked_input_ids.to(device)\n",
    "            mask = mask.to(device)\n",
    "            logits = model(masked_input_ids, mask, token_type_ids=None)\n",
    "            loss = cross_entropy_selected(logits, batch, mask)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            wandb.log({\"loss\": loss, \"lr\": lr})\n",
    "            opt.step()\n",
    "        t.save(model.state_dict(), f\"./models/{run_name}/epoch_{epoch}.pt\")\n",
    "    wandb.finish()\n",
    "\n",
    "if MAIN:\n",
    "    tiny_bert = BertLanguageModel(bert_config_tiny)\n",
    "    num_params = sum((p.nelement() for p in tiny_bert.parameters()))\n",
    "    print(\"Number of model parameters: \", num_params)\n",
    "    bert_mlm_pretrain(tiny_bert, config_dict, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
