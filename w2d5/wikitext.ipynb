{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.8/site-packages (4.24.0)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (1.12.1)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: fancy_einsum in /home/ubuntu/.local/lib/python3.8/site-packages (0.0.3)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/.local/lib/python3.8/site-packages (0.13.5)\n",
      "Requirement already satisfied: plotly in /home/ubuntu/.local/lib/python3.8/site-packages (5.11.0)\n",
      "Requirement already satisfied: torchinfo in /home/ubuntu/.local/lib/python3.8/site-packages (1.7.1)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.5.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (45.2.0)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (4.21.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/lib/python3/dist-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: pathtools in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb) (1.14.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from plotly) (8.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->transformers) (2.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip3 install transformers torch einops fancy_einsum wandb plotly torchinfo tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import utils\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import wandb\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "DATA_FOLDER = \"./data\"\n",
    "DATASET = \"2\"\n",
    "BASE_URL = \"https://s3.amazonaws.com/research.metamind.io/wikitext/\"\n",
    "DATASETS = {\"103\": \"wikitext-103-raw-v1.zip\", \"2\": \"wikitext-2-raw-v1.zip\"}\n",
    "TOKENS_FILENAME = os.path.join(DATA_FOLDER, f\"wikitext_tokens_{DATASET}.pt\")\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.mkdir(DATA_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url: str, path: str) -> None:\n",
    "    '''\n",
    "    Download the file from url and save it to path. \n",
    "    If path already exists, do nothing.\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {url} to {path}\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset WikiText-2 - options are 2 and 103\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_FOLDER, DATASETS[DATASET])\n",
    "maybe_download(BASE_URL + DATASETS[DATASET], path)\n",
    "expected_hexdigest = {\"103\": \"0ca3512bd7a238be4a63ce7b434f8935\", \"2\": \"f407a2d53283fc4a49bcff21bc5f3770\"}\n",
    "with open(path, \"rb\") as f:\n",
    "    actual_hexdigest = hashlib.md5(f.read()).hexdigest()\n",
    "    assert actual_hexdigest == expected_hexdigest[DATASET]\n",
    "\n",
    "print(f\"Using dataset WikiText-{DATASET} - options are 2 and 103\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "z = zipfile.ZipFile(path)\n",
    "\n",
    "def decompress(*splits: str) -> str:\n",
    "    return [\n",
    "        z.read(f\"wikitext-{DATASET}-raw/wiki.{split}.raw\").decode(\"utf-8\").splitlines()\n",
    "        for split in splits\n",
    "    ]\n",
    "\n",
    "train_text, val_text, test_text = decompress(\"train\", \"valid\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer Modules\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape is:  torch.Size([19159, 128])\n",
      "Tokenizing validation text...\n",
      "Tokenizing test text...\n",
      "Saving tokens to:  ./data/wikitext_tokens_2.pt\n"
     ]
    }
   ],
   "source": [
    "# Call the tokenizer on the list of lines with truncation=False to obtain lists of tokens. \n",
    "# These will be of varying length, and you'll notice some are empty due to blank lines.\n",
    "\n",
    "# Build one large 1D tensor containing all the tokens in sequence\n",
    "# Reshape the 1D tensor into (batch, sequence).\n",
    "\n",
    "def tokenize_1d(tokenizer, lines: List[str], max_seq: int) -> t.Tensor:\n",
    "    '''Tokenize text and rearrange into chunks of the maximum length.\n",
    "\n",
    "    Return (batch, seq) and an integer dtype.\n",
    "    '''\n",
    "    tokens = map(tokenizer.encode, lines) # tokenize\n",
    "    flattened = (item for sublist in tokens for item in sublist) # flatten\n",
    "    ten = t.tensor(list(flattened), dtype=t.int) # convert to tensor\n",
    "    ten = ten[: max_seq * (ten.shape[0] // max_seq)] # truncate to max_seq\n",
    "    return ten.reshape(-1, max_seq) # reshape to (batch, seq)\n",
    "\n",
    "if MAIN:\n",
    "    max_seq = 128\n",
    "    print(\"Tokenizing training text...\")\n",
    "    train_data = tokenize_1d(tokenizer, train_text, max_seq)\n",
    "    print(\"Training data shape is: \", train_data.shape)\n",
    "    print(\"Tokenizing validation text...\")\n",
    "    val_data = tokenize_1d(tokenizer, val_text, max_seq)\n",
    "    print(\"Tokenizing test text...\")\n",
    "    test_data = tokenize_1d(tokenizer, test_text, max_seq)\n",
    "    print(\"Saving tokens to: \", TOKENS_FILENAME)\n",
    "    t.save((train_data, val_data, test_data), TOKENS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing empirical frequencies\n",
      "Checking fraction of tokens selected...\n",
      "Checking fraction of tokens masked...\n",
      "Checking fraction of tokens masked OR randomized...\n"
     ]
    }
   ],
   "source": [
    "def random_mask(\n",
    "    input_ids: t.Tensor, mask_token_id: int, vocab_size: int, select_frac=0.15, mask_frac=0.8, random_frac=0.1\n",
    ") -> Tuple[t.Tensor, t.Tensor]:\n",
    "    '''Given a batch of tokens, return a copy with tokens replaced according to Section 3.1 of the paper.\n",
    "\n",
    "    input_ids: (batch, seq)\n",
    "\n",
    "    Return: (model_input, was_selected) where:\n",
    "\n",
    "    model_input: (batch, seq) - a new Tensor with the replacements made, suitable for passing to the BertLanguageModel. Don't modify the original tensor!\n",
    "\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "\n",
    "    we must mask 15% of tokens in each sequence\n",
    "    80% of those must be replaced with the [MASK] token\n",
    "    The remaining 10% of masked tokens can be replaced with any token from the vocabulary.\n",
    "    '''\n",
    "    d_batch, d_seq = input_ids.shape\n",
    "    n_masking = select_frac * d_seq * d_batch\n",
    "    n_mask = int(n_masking * mask_frac)\n",
    "    n_random = int(n_masking * random_frac)\n",
    "    n_unchanged = int(n_masking * (1 - mask_frac - random_frac))\n",
    "    # print(f'Out of {d_seq}, masking {n_mask} tokens with [MASK], {n_random} with random tokens, and {n_unchanged} unchanged')\n",
    "    # print(f'Frequency of [MASK] is {n_mask / d_seq}')\n",
    "    # print(f'Frequency of random is {n_random / d_seq}')\n",
    "    # print(f'Frequency of unchanged is {n_unchanged / d_seq}')\n",
    "    # choose which tokens to mask\n",
    "    mask = t.zeros(d_batch*d_seq, dtype=t.int)\n",
    "    mask[:n_mask] = 1\n",
    "    mask[n_mask:n_mask + n_random] = 2\n",
    "    mask[n_mask + n_random:n_mask + n_random + n_unchanged] = 3\n",
    "    mask = mask.reshape(d_batch, d_seq)\n",
    "    \n",
    "    # for the input ids replace them with masked token where mask = 1\n",
    "    # and with random token where mask = 2\n",
    "    model_input = input_ids.clone()\n",
    "    model_input[mask == 1] = mask_token_id\n",
    "    rand = t.randint(0, vocab_size, (d_batch, d_seq))\n",
    "    model_input = t.where(mask == 2, rand, model_input)\n",
    "    mask = t.where(mask != 0, 1, 0)\n",
    "\n",
    "    return model_input, mask\n",
    "    \n",
    "# test mask\n",
    "\n",
    "# print decoded tokens\n",
    "# print(tokenizer.decode(train_data[:2].flatten().tolist()))\n",
    "out1, mask1 = random_mask(train_data[:2], tokenizer.mask_token_id, tokenizer.vocab_size, select_frac=0.85, mask_frac=0.1, random_frac=0.8)\n",
    "# print(tokenizer.decode(out1.flatten().tolist()))\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_random_mask(random_mask, input_size=10000, max_seq=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2800)\n"
     ]
    }
   ],
   "source": [
    "# Find the word frequencies\n",
    "word_frequencies = t.bincount(train_data.flatten())\n",
    "# Drop the words with occurrence zero (because these contribute zero to cross entropy)\n",
    "word_frequencies = word_frequencies[word_frequencies > 0]\n",
    "# Get probabilities\n",
    "word_probabilities = word_frequencies / word_frequencies.sum()\n",
    "# Calculate the cross entropy\n",
    "cross_entropy = (- word_probabilities * word_probabilities.log()).sum()\n",
    "print(cross_entropy)\n",
    "# ==> 7.3446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random MLM loss on random tokens - does this make sense? 10.32\n"
     ]
    }
   ],
   "source": [
    "def flat(x: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Combines batch and sequence dimensions.\"\"\"\n",
    "    return rearrange(x, \"b s ... -> (b s) ...\")\n",
    "\n",
    "def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    pred: (batch, seq, vocab_size) - predictions from the model\n",
    "    target: (batch, seq, ) - the original (not masked) input ids\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "    Out: the mean loss per predicted token\n",
    "    '''\n",
    "    # select correct predictions from the predictions\n",
    "    target = t.where(was_selected == 1, target, -100)\n",
    "    entropy = F.cross_entropy(flat(pred), flat(target.long()))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_cross_entropy_selected(cross_entropy_selected)\n",
    "\n",
    "    batch_size = 8\n",
    "    seq_length = 512\n",
    "    batch = t.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "    pred = t.rand((batch_size, seq_length, tokenizer.vocab_size))\n",
    "    (masked, was_selected) = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "    loss = cross_entropy_selected(pred, batch, was_selected).item()\n",
    "    print(f\"Random MLM loss on random tokens - does this make sense? {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  torch.Size([19159, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "hidden_size = 512\n",
    "bert_config_tiny = TransformerConfig(\n",
    "    num_layers = 8,\n",
    "    num_heads = hidden_size // 64,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = hidden_size,\n",
    "    max_seq_len = 128,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.0002,\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "(train_data, val_data, test_data) = t.load(\"./data/wikitext_tokens_2.pt\")\n",
    "print(\"Training data size: \", train_data.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_data), shuffle=True, batch_size=config_dict[\"batch_size\"], drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from fancy_einsum import einsum\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from dataclasses import dataclass \n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from typing import Optional, Union\n",
    "import plotly.express as px\n",
    "import torchinfo\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import transformers\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import utils\n",
    "\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "bertconfig = TransformerConfig(\n",
    "    num_layers = 12,\n",
    "    num_heads = 12,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = 768,\n",
    "    max_seq_len = 512,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "class MultiLayerPerceptron(nn.Module):  \n",
    "\n",
    "    def __init__(self, d_in: int, d_out: int):\n",
    "        super().__init__()\n",
    "        d_h = d_in * 4\n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(d_in, d_h)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(d_h, d_in)),   \n",
    "            ('dropout', nn.Dropout(p=0.1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.model(x)\n",
    "\n",
    "class MultiheadMaskedAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.W_QKV = nn.Linear(hidden_size, hidden_size * 3)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x: t.Tensor, mask=None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        Return: shape (batch, seq, hidden_size)\n",
    "        '''\n",
    "        Q, K, V = self.W_QKV(x).chunk(3, dim=-1)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads s1 h, b nheads s2 h -> b nheads s2 s1', K, Q) / (headsize ** 0.5)\n",
    "        mask_filter = t.triu(t.full_like(scaled_dot_prod, -t.inf), 1)\n",
    "        scaled_dot_prod += mask_filter\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_probs = self.dropout1(attention_probs)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return self.dropout2(attention) \n",
    "\n",
    "class GPT2DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.attention = MultiheadMaskedAttention(\n",
    "            hidden_size=config.hidden_size,\n",
    "            num_heads=config.num_heads\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = MultiLayerPerceptron(config.hidden_size, config.hidden_size)\n",
    "    \n",
    "    def forward(self, x: t.Tensor):\n",
    "        x = x + self.attention(self.layernorm1(x))\n",
    "        x = x + self.mlp(self.layernorm2(x))\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.positional_encoding = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        decoders = [GPT2DecoderBlock(config) for i in range(config.num_layers)]\n",
    "        names = ['decoder' + str(i) for i in range(config.num_layers)]\n",
    "        self.decoderlayer = nn.Sequential(OrderedDict(zip(names, decoders)))\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.layernorm = nn.LayerNorm(config.hidden_size)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        if len(tokens.shape) == 1:\n",
    "            tokens = rearrange(tokens, \"seq -> 1 seq\")\n",
    "        embedding = self.embed(tokens) # (b, seq_len) -> (b, seq_len, embedding)\n",
    "        pos_enc = self.positional_encoding(t.arange(tokens.shape[1], device=tokens.device)) # (seq_len)\n",
    "        a = self.dropout(embedding + pos_enc) # (b, seq_len, embedding)\n",
    "        b = self.decoderlayer(a) # (b, seq_len, embedding)\n",
    "        c = self.layernorm(b) @ self.embed.weight.T # (b, seq_len, embedding) @ (embedding, vocab_size) -> (b, seq_len, vocab_size)\n",
    "        return c\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hidden_size, num_heads = config.hidden_size, config.num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)\n",
    "        if additive_attention_mask is not None:\n",
    "            scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return attention\n",
    "\n",
    "class BERTMLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs, p_dropout = config.hidden_size, config.dropout\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(hs, hs * 4)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(hs * 4, hs)),   \n",
    "            ('dropout', nn.Dropout(p_dropout))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = BERTMLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        h1 = self.ln1(self.attention(x, additive_attention_mask) + x) # TODO chain this\n",
    "        h2 = self.ln2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')\n",
    "\n",
    "# util one is erroring\n",
    "def test_make_additive_attention_mask(make_additive_attention_mask):\n",
    "    def make_additive_attention_mask_soln(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "        '''\n",
    "        one_zero_attention_mask: \n",
    "            shape (batch, seq)\n",
    "            Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "        big_negative_number:\n",
    "            Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "        Out: shape (batch, heads, seq, seq). Contains 0 if attention is allowed, and big_negative_number if it is not allowed.\n",
    "        '''\n",
    "        return big_negative_number * repeat(1 - one_zero_attention_mask, \"batch seqK -> batch 1 1 seqK\")\n",
    "    arr = t.randint(low=0, high=2, size=(3, 4))\n",
    "    expected = make_additive_attention_mask_soln(arr)\n",
    "    actual = make_additive_attention_mask(arr)\n",
    "    t.testing.assert_close(expected, actual)\n",
    "\n",
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tokentype_emb = nn.Embedding(2, config.hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        positional_embedding = self.pos_emb(t.arange(seq_len)) # (seq_len, emb)\n",
    "        token_type_ids = token_type_ids if token_type_ids else t.zeros_like(input_ids)\n",
    "        token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)\n",
    "        x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))\n",
    "        mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask is not None else None\n",
    "        for block in self.bertblocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs = config.hidden_size\n",
    "        self.bertcommon = BertCommon(config)\n",
    "        self.linear = nn.Linear(hs, hs)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        xavier = 1 / (config.vocab_size ** 0.5)\n",
    "        self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        unembed = self.bertcommon.token_emb.weight.T\n",
    "        x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n",
    "        x = self.linear(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.ln(x)\n",
    "        return x @ unembed + self.unembed_bias\n",
    "\n",
    "my_bert = BertLanguageModel(bertconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size:  torch.Size([19159, 128])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "hidden_size = 512\n",
    "bert_config_tiny = TransformerConfig(\n",
    "    num_layers = 8,\n",
    "    num_heads = hidden_size // 64,\n",
    "    vocab_size = 28996,\n",
    "    hidden_size = hidden_size,\n",
    "    max_seq_len = 128,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.0002,\n",
    "    epochs=40,\n",
    "    batch_size=128,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "(train_data, val_data, test_data) = t.load(\"./data/wikitext_tokens_2.pt\")\n",
    "print(\"Training data size: \", train_data.shape)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(train_data), shuffle=True, batch_size=config_dict[\"batch_size\"], drop_last=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model: BertLanguageModel, config_dict: dict) -> t.optim.AdamW:\n",
    "    '''\n",
    "    Loop over model parameters and form two parameter groups:\n",
    "\n",
    "    - The first group includes the weights of each Linear layer and uses the weight decay in config_dict\n",
    "    - The second has all other parameters and uses weight decay of 0\n",
    "    '''\n",
    "    params1 = []\n",
    "    params2 = []\n",
    "    matches = ['W_O.weight', 'W_V.weight', 'W_Q.weight', 'W_K.weight', 'linear1.weight', 'linear2.weight', 'linear.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if any([match in name for match in matches]):\n",
    "            params1.append(param)\n",
    "        else:\n",
    "            params2.append(param)\n",
    "    \n",
    "    # \n",
    "    # \n",
    "    params = [\n",
    "        {'params': params1, 'weight_decay': config_dict['weight_decay']},\n",
    "        {'params': params2, 'weight_decay': 0, **config_dict}\n",
    "    ]\n",
    "    return t.optim.AdamW(params, lr=config_dict['lr'])\n",
    "    \n",
    "\n",
    "if MAIN:\n",
    "    test_config = TransformerConfig(\n",
    "        num_layers = 3,\n",
    "        num_heads = 1,\n",
    "        vocab_size = 28996,\n",
    "        hidden_size = 1,\n",
    "        max_seq_len = 4,\n",
    "        dropout = 0.1,\n",
    "        layer_norm_epsilon = 1e-12,\n",
    "    )\n",
    "\n",
    "    optimizer_test_model = BertLanguageModel(test_config)\n",
    "    opt = make_optimizer(\n",
    "        optimizer_test_model, \n",
    "        dict(weight_decay=0.1, lr=0.0001, eps=1e-06)\n",
    "    )\n",
    "    expected_num_with_weight_decay = test_config.num_layers * 6 + 1\n",
    "    wd_group = opt.param_groups[0]\n",
    "    actual = len(wd_group[\"params\"])\n",
    "    assert (\n",
    "        actual == expected_num_with_weight_decay\n",
    "    ), f\"Expected 6 linear weights per layer (4 attn, 2 MLP) plus the final lm_linear weight to have weight decay, got {actual}\"\n",
    "    all_params = set()\n",
    "    for group in opt.param_groups:\n",
    "        all_params.update(group[\"params\"])\n",
    "    assert all_params == set(optimizer_test_model.parameters()), \"Not all parameters were passed to optimizer!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0\n",
      "Epoch 0, batch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m             opt\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m bert_mlm_pretrain(my_bert, \u001b[39mdict\u001b[39;49m(weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.0001\u001b[39;49m, eps\u001b[39m=\u001b[39;49m\u001b[39m1e-06\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), train_loader)\n\u001b[1;32m     31\u001b[0m \u001b[39mif\u001b[39;00m MAIN:\n\u001b[1;32m     32\u001b[0m     model \u001b[39m=\u001b[39m BertLanguageModel(bert_config_tiny)\n",
      "Cell \u001b[0;32mIn [13], line 16\u001b[0m, in \u001b[0;36mbert_mlm_pretrain\u001b[0;34m(model, config_dict, train_loader)\u001b[0m\n\u001b[1;32m     14\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m masked_input_ids, mask \u001b[39m=\u001b[39m random_mask(batch, tokenizer\u001b[39m.\u001b[39mmask_token_id, tokenizer\u001b[39m.\u001b[39mvocab_size)\n\u001b[0;32m---> 16\u001b[0m logits \u001b[39m=\u001b[39m model(masked_input_ids, mask, token_type_ids\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     17\u001b[0m \u001b[39m# def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# pred: (batch, seq, vocab_size) - predictions from the model\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# target: (batch, seq, ) - the original (not masked) input ids\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m# Out: the mean loss per predicted token\u001b[39;00m\n\u001b[1;32m     23\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_selected(logits, batch, mask)\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 311\u001b[0m, in \u001b[0;36mBertLanguageModel.forward\u001b[0;34m(self, input_ids, one_zero_attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39minput_ids: (batch, seq) - the token ids\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39mone_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mtoken_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    310\u001b[0m unembed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbertcommon\u001b[39m.\u001b[39mtoken_emb\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 311\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n\u001b[1;32m    312\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(x)\n\u001b[1;32m    313\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 284\u001b[0m, in \u001b[0;36mBertCommon.forward\u001b[0;34m(self, input_ids, one_zero_attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m    282\u001b[0m mask \u001b[39m=\u001b[39m make_additive_attention_mask(one_zero_attention_mask) \u001b[39mif\u001b[39;00m one_zero_attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbertblocks:\n\u001b[0;32m--> 284\u001b[0m     x \u001b[39m=\u001b[39m block(x, mask)\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 217\u001b[0m, in \u001b[0;36mBERTBlock.forward\u001b[0;34m(self, x, additive_attention_mask)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39mx: shape (batch, seq, hidden_size)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[39madditive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    216\u001b[0m h1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(x, additive_attention_mask) \u001b[39m+\u001b[39m x) \u001b[39m# TODO chain this\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m h2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(h1) \u001b[39m+\u001b[39m h1)\n\u001b[1;32m    218\u001b[0m \u001b[39mreturn\u001b[39;00m h2\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [10], line 200\u001b[0m, in \u001b[0;36mBERTMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: t\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(x)\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/activation.py:681\u001b[0m, in \u001b[0;36mGELU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 681\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgelu(\u001b[39minput\u001b[39;49m, approximate\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapproximate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def bert_mlm_pretrain(model: BertLanguageModel, config_dict: dict, train_loader: DataLoader) -> None:\n",
    "    '''Train using masked language modelling.'''\n",
    "    model.train()\n",
    "    opt = make_optimizer(model, config_dict)\n",
    "    wandb.init(project=\"bert\", config=config_dict)\n",
    "    wandb.watch(model)\n",
    "    for epoch in range(config_dict['epochs']):\n",
    "        for n_batch, (batch,) in enumerate(train_loader):\n",
    "            wandb.log({\"epoch\": epoch, \"batch\": n_batch})\n",
    "            opt.zero_grad()\n",
    "            masked_input_ids, mask = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "            logits = model(masked_input_ids, mask, token_type_ids=None)\n",
    "            # def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\n",
    "            # pred: (batch, seq, vocab_size) - predictions from the model\n",
    "            # target: (batch, seq, ) - the original (not masked) input ids\n",
    "            # was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "            # Out: the mean loss per predicted token\n",
    "            loss = cross_entropy_selected(logits, batch, mask)\n",
    "            loss.backward()\n",
    "            wandb.log({\"loss\": loss})\n",
    "            opt.step()\n",
    "        print(f'Epoch {epoch} loss: {loss.item()}')\n",
    "    wandb.finish()\n",
    "    \n",
    "tiny_bert = BertLanguageModel(bert_config_tiny)\n",
    "bert_mlm_pretrain(tiny_bert, config_dict, train_loader)\n",
    "\n",
    "if MAIN:\n",
    "    model = BertLanguageModel(bert_config_tiny)\n",
    "    num_params = sum((p.nelement() for p in model.parameters()))\n",
    "    print(\"Number of model parameters: \", num_params)\n",
    "    bert_mlm_pretrain(model, config_dict, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
