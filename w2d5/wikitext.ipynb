{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "import transformers\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import utils\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "DATA_FOLDER = \"./data\"\n",
    "DATASET = \"2\"\n",
    "BASE_URL = \"https://s3.amazonaws.com/research.metamind.io/wikitext/\"\n",
    "DATASETS = {\"103\": \"wikitext-103-raw-v1.zip\", \"2\": \"wikitext-2-raw-v1.zip\"}\n",
    "TOKENS_FILENAME = os.path.join(DATA_FOLDER, f\"wikitext_tokens_{DATASET}.pt\")\n",
    "\n",
    "if not os.path.exists(DATA_FOLDER):\n",
    "    os.mkdir(DATA_FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(url: str, path: str) -> None:\n",
    "    '''\n",
    "    Download the file from url and save it to path. \n",
    "    If path already exists, do nothing.\n",
    "    '''\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Downloading {url} to {path}\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(requests.get(url).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip to ./data/wikitext-2-raw-v1.zip\n",
      "Using dataset WikiText-2 - options are 2 and 103\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(DATA_FOLDER, DATASETS[DATASET])\n",
    "maybe_download(BASE_URL + DATASETS[DATASET], path)\n",
    "expected_hexdigest = {\"103\": \"0ca3512bd7a238be4a63ce7b434f8935\", \"2\": \"f407a2d53283fc4a49bcff21bc5f3770\"}\n",
    "with open(path, \"rb\") as f:\n",
    "    actual_hexdigest = hashlib.md5(f.read()).hexdigest()\n",
    "    assert actual_hexdigest == expected_hexdigest[DATASET]\n",
    "\n",
    "print(f\"Using dataset WikiText-{DATASET} - options are 2 and 103\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "z = zipfile.ZipFile(path)\n",
    "\n",
    "def decompress(*splits: str) -> str:\n",
    "    return [\n",
    "        z.read(f\"wikitext-{DATASET}-raw/wiki.{split}.raw\").decode(\"utf-8\").splitlines()\n",
    "        for split in splits\n",
    "    ]\n",
    "\n",
    "train_text, val_text, test_text = decompress(\"train\", \"valid\", \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' = Valkyria Chronicles III = ',\n",
       " ' ',\n",
       " ' Senjō no Valkyria 3 : Unrecorded Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . ',\n",
       " \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_text))\n",
    "train_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing training text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (686 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape is:  torch.Size([19159, 128])\n",
      "Tokenizing validation text...\n",
      "Tokenizing test text...\n",
      "Saving tokens to:  ./data/wikitext_tokens_2.pt\n"
     ]
    }
   ],
   "source": [
    "# Call the tokenizer on the list of lines with truncation=False to obtain lists of tokens. \n",
    "# These will be of varying length, and you'll notice some are empty due to blank lines.\n",
    "\n",
    "# Build one large 1D tensor containing all the tokens in sequence\n",
    "# Reshape the 1D tensor into (batch, sequence).\n",
    "\n",
    "def tokenize_1d(tokenizer, lines: list[str], max_seq: int) -> t.Tensor:\n",
    "    '''Tokenize text and rearrange into chunks of the maximum length.\n",
    "\n",
    "    Return (batch, seq) and an integer dtype.\n",
    "    '''\n",
    "    tokens = map(tokenizer.encode, lines) # tokenize\n",
    "    flattened = (item for sublist in tokens for item in sublist) # flatten\n",
    "    ten = t.tensor(list(flattened), dtype=t.int) # convert to tensor\n",
    "    ten = ten[: max_seq * (ten.shape[0] // max_seq)] # truncate to max_seq\n",
    "    return ten.reshape(-1, max_seq) # reshape to (batch, seq)\n",
    "\n",
    "if MAIN:\n",
    "    max_seq = 128\n",
    "    print(\"Tokenizing training text...\")\n",
    "    train_data = tokenize_1d(tokenizer, train_text, max_seq)\n",
    "    print(\"Training data shape is: \", train_data.shape)\n",
    "    print(\"Tokenizing validation text...\")\n",
    "    val_data = tokenize_1d(tokenizer, val_text, max_seq)\n",
    "    print(\"Tokenizing test text...\")\n",
    "    test_data = tokenize_1d(tokenizer, test_text, max_seq)\n",
    "    print(\"Saving tokens to: \", TOKENS_FILENAME)\n",
    "    t.save((train_data, val_data, test_data), TOKENS_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing empirical frequencies\n",
      "Checking fraction of tokens selected...\n",
      "Checking fraction of tokens masked...\n",
      "Checking fraction of tokens masked OR randomized...\n"
     ]
    }
   ],
   "source": [
    "def random_mask(\n",
    "    input_ids: t.Tensor, mask_token_id: int, vocab_size: int, select_frac=0.15, mask_frac=0.8, random_frac=0.1\n",
    ") -> tuple[t.Tensor, t.Tensor]:\n",
    "    '''Given a batch of tokens, return a copy with tokens replaced according to Section 3.1 of the paper.\n",
    "\n",
    "    input_ids: (batch, seq)\n",
    "\n",
    "    Return: (model_input, was_selected) where:\n",
    "\n",
    "    model_input: (batch, seq) - a new Tensor with the replacements made, suitable for passing to the BertLanguageModel. Don't modify the original tensor!\n",
    "\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "\n",
    "    we must mask 15% of tokens in each sequence\n",
    "    80% of those must be replaced with the [MASK] token\n",
    "    The remaining 10% of masked tokens can be replaced with any token from the vocabulary.\n",
    "    '''\n",
    "    d_batch, d_seq = input_ids.shape\n",
    "    n_masking = select_frac * d_seq * d_batch\n",
    "    n_mask = int(n_masking * mask_frac)\n",
    "    n_random = int(n_masking * random_frac)\n",
    "    n_unchanged = int(n_masking * (1 - mask_frac - random_frac))\n",
    "    # print(f'Out of {d_seq}, masking {n_mask} tokens with [MASK], {n_random} with random tokens, and {n_unchanged} unchanged')\n",
    "    # print(f'Frequency of [MASK] is {n_mask / d_seq}')\n",
    "    # print(f'Frequency of random is {n_random / d_seq}')\n",
    "    # print(f'Frequency of unchanged is {n_unchanged / d_seq}')\n",
    "    # choose which tokens to mask\n",
    "    mask = t.zeros(d_batch*d_seq, dtype=t.int)\n",
    "    mask[:n_mask] = 1\n",
    "    mask[n_mask:n_mask + n_random] = 2\n",
    "    mask[n_mask + n_random:n_mask + n_random + n_unchanged] = 3\n",
    "    mask = mask.reshape(d_batch, d_seq)\n",
    "    \n",
    "    # for the input ids replace them with masked token where mask = 1\n",
    "    # and with random token where mask = 2\n",
    "    model_input = input_ids.clone()\n",
    "    model_input[mask == 1] = mask_token_id\n",
    "    rand = t.randint(0, vocab_size, (d_batch, d_seq))\n",
    "    model_input = t.where(mask == 2, rand, model_input)\n",
    "    mask = t.where(mask != 0, 1, 0)\n",
    "\n",
    "    return model_input, mask\n",
    "    \n",
    "# test mask\n",
    "\n",
    "# print decoded tokens\n",
    "# print(tokenizer.decode(train_data[:2].flatten().tolist()))\n",
    "out1, mask1 = random_mask(train_data[:2], tokenizer.mask_token_id, tokenizer.vocab_size, select_frac=0.85, mask_frac=0.1, random_frac=0.8)\n",
    "# print(tokenizer.decode(out1.flatten().tolist()))\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_random_mask(random_mask, input_size=10000, max_seq=max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2800)\n"
     ]
    }
   ],
   "source": [
    "# Find the word frequencies\n",
    "word_frequencies = t.bincount(train_data.flatten())\n",
    "# Drop the words with occurrence zero (because these contribute zero to cross entropy)\n",
    "word_frequencies = word_frequencies[word_frequencies > 0]\n",
    "# Get probabilities\n",
    "word_probabilities = word_frequencies / word_frequencies.sum()\n",
    "# Calculate the cross entropy\n",
    "cross_entropy = (- word_probabilities * word_probabilities.log()).sum()\n",
    "print(cross_entropy)\n",
    "# ==> 7.3446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random MLM loss on random tokens - does this make sense? 10.32\n"
     ]
    }
   ],
   "source": [
    "def flat(x: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Combines batch and sequence dimensions.\"\"\"\n",
    "    return rearrange(x, \"b s ... -> (b s) ...\")\n",
    "\n",
    "def cross_entropy_selected(pred: t.Tensor, target: t.Tensor, was_selected: t.Tensor) -> t.Tensor:\n",
    "    '''\n",
    "    pred: (batch, seq, vocab_size) - predictions from the model\n",
    "    target: (batch, seq, ) - the original (not masked) input ids\n",
    "    was_selected: (batch, seq) - 1 if the token at this index will contribute to the MLM loss, 0 otherwise\n",
    "\n",
    "    Out: the mean loss per predicted token\n",
    "    '''\n",
    "    # select correct predictions from the predictions\n",
    "    target = t.where(was_selected == 1, target, -100)\n",
    "    entropy = F.cross_entropy(flat(pred), flat(target))\n",
    "    return entropy\n",
    "\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_cross_entropy_selected(cross_entropy_selected)\n",
    "\n",
    "    batch_size = 8\n",
    "    seq_length = 512\n",
    "    batch = t.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "    pred = t.rand((batch_size, seq_length, tokenizer.vocab_size))\n",
    "    (masked, was_selected) = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "    loss = cross_entropy_selected(pred, batch, was_selected).item()\n",
    "    print(f\"Random MLM loss on random tokens - does this make sense? {loss:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
